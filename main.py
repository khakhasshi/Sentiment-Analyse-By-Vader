import praw
import prawcore
import pandas as pd
import matplotlib.pyplot as plt
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from tqdm import tqdm
import platform  # Detect operating system
import time
import datetime  # å¤„ç†æ—¶é—´

# === Reddit API Configuration ===
REDDIT_CLIENT_ID = ""
REDDIT_CLIENT_SECRET = ""
REDDIT_USER_AGENT = ""

# === Initialize Sentiment Analyzer ===
analyzer = SentimentIntensityAnalyzer()

# === Fix Matplotlib Font Issues (for macOS) ===
if platform.system() == "Darwin":  # macOS
    plt.rcParams["font.sans-serif"] = ["Arial Unicode MS"]  # Use macOS-compatible font
else:  # Windows/Linux
    plt.rcParams["font.sans-serif"] = ["Arial"]
plt.rcParams["axes.unicode_minus"] = False  # Ensure minus signs display correctly

# è®¾ç½®æ—¶é—´é™åˆ¶ï¼ˆ2025å¹´3æœˆ1æ—¥ 00:00 UTCï¼‰
TIME_THRESHOLD = datetime.datetime(2025, 1,5).timestamp()

def fetch_posts_by_keywords(subreddit_name, keywords, max_posts=500, search_limit=1000):
    """ é€šè¿‡å¤šä¸ªå…³é”®è¯æœç´¢ç¬¦åˆæ—¶é—´èŒƒå›´çš„å¸–å­ï¼ˆ2025å¹´3æœˆ1æ—¥åŽï¼‰ """
    try:
        reddit = praw.Reddit(
            client_id=REDDIT_CLIENT_ID,
            client_secret=REDDIT_CLIENT_SECRET,
            user_agent=REDDIT_USER_AGENT,
        )

        post_urls = set()
        with tqdm(total=max_posts, desc="ðŸ” æœç´¢ç¬¦åˆæ—¶é—´è¦æ±‚çš„å¸–å­") as pbar:
            for keyword in keywords:
                count = 0  # ç»Ÿè®¡ç¬¦åˆæ¡ä»¶çš„å¸–å­æ•°é‡
                for submission in reddit.subreddit(subreddit_name).search(keyword, limit=search_limit):
                    # è¿‡æ»¤æŽ‰æ—¶é—´å°äºŽ2025å¹´3æœˆ1æ—¥çš„å¸–å­
                    if submission.created_utc >= TIME_THRESHOLD and submission.url.startswith("https://www.reddit.com/r/"):
                        post_urls.add(submission.url)
                        count += 1
                        pbar.update(1)

                    if len(post_urls) >= max_posts:
                        break  # è¾¾åˆ° 300 æ¡å¸–å­ï¼Œåœæ­¢æœç´¢
                
                if len(post_urls) >= max_posts:
                    break  # è¾¾åˆ° 300 æ¡å¸–å­ï¼Œåœæ­¢æœç´¢
                
                time.sleep(0.1)  # é˜²æ­¢ API é€ŸçŽ‡é™åˆ¶

        print(f"\nâœ… å…±èŽ·å– {len(post_urls)} ä¸ªç¬¦åˆæ—¶é—´è¦æ±‚çš„å¸–å­")
        return list(post_urls)

    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        return []

def fetch_post_and_comments(post_url):
    """ èŽ·å– Reddit å¸–å­åŠå…¶è¯„è®º """
    try:
        reddit = praw.Reddit(
            client_id=REDDIT_CLIENT_ID,
            client_secret=REDDIT_CLIENT_SECRET,
            user_agent=REDDIT_USER_AGENT,
        )

        submission = reddit.submission(url=post_url)

        # æ‰“å°å¸–å­ä¿¡æ¯
        print(f"\nðŸ“¥ å¤„ç†å¸–å­: {submission.title}")
        print(f"ðŸ‘¤ ä½œè€…: {submission.author} | ðŸ‘ {submission.score} | ðŸ’¬ {submission.num_comments} æ¡è¯„è®º")

        submission.comments.replace_more(limit=0)
        comments_data = []

        for comment in submission.comments.list():
            if comment.body:
                comments_data.append([comment.author, comment.body, comment.score])

        if not comments_data:
            print("âš ï¸ æ²¡æœ‰å¯ç”¨è¯„è®º")
            return None

        df = pd.DataFrame(comments_data, columns=["Author", "Comment", "Upvotes"])
        return df, submission.id

    except prawcore.exceptions.NotFound:
        print("âŒ å¸–å­æœªæ‰¾åˆ°æˆ–å·²åˆ é™¤")
    except prawcore.exceptions.Forbidden:
        print("âŒ æ— æƒé™è®¿é—®è¯¥å¸–å­ï¼ˆå¯èƒ½æ˜¯ç§å¯†å¸–å­ï¼‰")
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
    return None

def analyze_sentiment(comment):
    """ è®¡ç®—æƒ…ç»ªå¾—åˆ†å¹¶åˆ†ç±» """
    sentiment_score = analyzer.polarity_scores(str(comment))["compound"]
    if sentiment_score >= 0.4:
        return 1  # Bullish
    elif sentiment_score <= -0.4:
        return -1  # Bearish
    else:
        return 0  # Neutral

def process_multiple_posts(post_urls):
    """ å¤„ç†å¤šä¸ªå¸–å­ï¼Œåˆ†æžè¯„è®ºæƒ…ç»ªå¹¶å¯è§†åŒ– """
    all_data = []

    with tqdm(total=len(post_urls), desc="ðŸ“Š å¤„ç†å¸–å­") as pbar:
        for post_url in post_urls:
            result = fetch_post_and_comments(post_url)
            if result:
                df, post_id = result
                tqdm.pandas(desc=f"ðŸ”„ åˆ†æžæƒ…ç»ª {post_id}")
                df["Sentiment_Label"] = df["Comment"].progress_apply(analyze_sentiment)
                df["Post_ID"] = post_id
                df["Post_URL"] = post_url
                all_data.append(df)
            pbar.update(1)

    if all_data:
        final_df = pd.concat(all_data, ignore_index=True)
        filename = "reddit_nvda_sentiment_analysis_2025_mar.csv"
        final_df.to_csv(filename, index=False, encoding="utf-8")
        print(f"\nâœ… æ‰€æœ‰å¸–å­åˆ†æžå®Œæˆï¼Œç»“æžœå·²ä¿å­˜åˆ° {filename}")

        # ç»Ÿè®¡æƒ…ç»ªåˆ†æžç»“æžœ
        sentiment_counts = final_df["Sentiment_Label"].value_counts().sort_index()

        # å¯è§†åŒ–ç»“æžœ
        plt.figure(figsize=(6, 4))
        plt.bar(sentiment_counts.index, sentiment_counts.values, tick_label=["Bearish (-1)", "Neutral (0)", "Bullish (1)"])
        plt.xlabel("Sentiment Category")
        plt.ylabel("Number of Comments")
        plt.title("NVDA ç›¸å…³æƒ…ç»ªåˆ†æž (2025å¹´3æœˆä¹‹åŽ)")
        plt.show()

# === è¿è¡Œä»£ç  ===
if __name__ == "__main__":
    subreddit_name = "NvidiaStock"  # ä½ å¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»– Subreddit
    keywords = ["NVDA", "nvidia", "Nvidia", "NVIDIA"]
    
    post_urls = fetch_posts_by_keywords(subreddit_name, keywords, max_posts=300, search_limit=1000)
    process_multiple_posts(post_urls)
